{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_6088118.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JJAiFeGsEeB",
        "colab_type": "text"
      },
      "source": [
        "# NLP Assignment\n",
        "Nontapat Pintira\n",
        "Student ID: 6088118\n",
        "\n",
        "## Analysis on Amazon Reviews\n",
        "This dataset consists of a few million Amazon customer reviews (input text) and star ratings (output labels) for learning how to train fastText for sentiment analysis.\n",
        "\n",
        "[Dataset on Google Drive](https://drive.google.com/open?id=1YJ8WU-4o31ehA7mSD_7Pv1EJeZwLByls)\n",
        "\n",
        "I am going to use RNN to create a model for this prediction task.\n",
        "Noted that the model this notebook takes can take around 30 minutes to train\n",
        "\n",
        "In total this notebook may takes up to 2 hours to run from start to end.\n",
        "(on Google Colab with GPU runtime)\n",
        "\n",
        "**Please run this notebook on Google Colab**\n",
        "\n",
        "## Library Used\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lIqwi2TtVRn",
        "colab_type": "code",
        "outputId": "3d71a186-4b91-4ae4-ad6d-7b90035a5e87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.keras import models, layers, optimizers\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import bz2\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Check the content in the mounted directory\n",
        "import os\n",
        "print(os.listdir(\"../content/drive/My Drive/Skill Tree/CS - Special Topic/NLP/datasets\"))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['test.ft.txt.bz2', 'train.ft.txt.bz2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dqavfgZt0zw",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "## Preprocessing\n",
        "\n",
        "### Reading the Text\n",
        "The text files used in this assignment are compressed in bzip2 format. <br>\n",
        "To deal with this, I am going to use bz2 library to read the text line by line\n",
        "\n",
        "```\n",
        "__label__<X> __label__<Y> ... <Text>\n",
        "```\n",
        "\n",
        "After the inspection of the dataset, I see that the label is the first word in each review and the rest are the text data. Moreover, I am going to encoded the label to integer of 0 and 1 (0: negative, 1: positive)\n",
        "\n",
        "Be sure to mount the notebook with Google Drive data source and configure the path to the tar.bz2 file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZRzYCf4uZkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_labels_and_texts(file):\n",
        "    labels = []\n",
        "    texts = []\n",
        "    for line in bz2.BZ2File(file):\n",
        "        x = line.decode(\"utf-8\")\n",
        "        labels.append(int(x[9]) - 1)\n",
        "        texts.append(x[10:].strip())\n",
        "    return np.array(labels), texts\n",
        "\n",
        "train_labels, train_texts = get_labels_and_texts('../content/drive/My Drive/Skill Tree/CS - Special Topic/NLP/datasets/train.ft.txt.bz2')\n",
        "test_labels, test_texts = get_labels_and_texts('../content/drive/My Drive/Skill Tree/CS - Special Topic/NLP/datasets/test.ft.txt.bz2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blYhnnij3M_j",
        "colab_type": "text"
      },
      "source": [
        "### Text Preprocessing\n",
        "I am going to use regex to process the text data and create a sequence for further processing.\n",
        "\n",
        "The steps that I will use to process the text data are as follows\n",
        "\n",
        "\n",
        "1. Lowercasing everything\n",
        "2. Removing non-word characters\n",
        "3. Removing non-ascii characters\n",
        "\n",
        "To use regular expression in python, I have to import re library\n",
        "\n",
        "\n",
        "```\n",
        "import re\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Sdzcvm63DHr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NON_ALPHANUM = re.compile(r'[\\W]')\n",
        "NON_ASCII = re.compile(r'[^a-z0-1\\s]')\n",
        "def normalize_texts(texts):\n",
        "    normalized_texts = []\n",
        "    for text in texts:\n",
        "        lower = text.lower()\n",
        "        no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n",
        "        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n",
        "        normalized_texts.append(no_non_ascii)\n",
        "    return normalized_texts\n",
        "        \n",
        "train_texts = normalize_texts(train_texts)\n",
        "test_texts = normalize_texts(test_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SrQdm4G86EP",
        "colab_type": "text"
      },
      "source": [
        "### Spliting Testing and Training Data\n",
        "\n",
        "Out of almost 4 million training examples, I will use development set of 250,000 examples (around 7%) to give enough confidence level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkpObtaN85bZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, random_state=42, test_size=0.07)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARzWwsxf9N51",
        "colab_type": "text"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "I will only use top 12,000 words as features.<br>\n",
        "Keras makes it easier to tokenize the text and create useful sequence for deep learning models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6sSndx79qbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_FEATURES = 12000\n",
        "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "train_texts = tokenizer.texts_to_sequences(train_texts)\n",
        "val_texts = tokenizer.texts_to_sequences(val_texts)\n",
        "test_texts = tokenizer.texts_to_sequences(test_texts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xisBR40x_WYR",
        "colab_type": "text"
      },
      "source": [
        "To use minibatch in the deep learning model, it is better to pad the input sequence to have equal size. Additionally, I have pad the data on each dataset (training, validation, testing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9hAzSGK_kNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)\n",
        "train_texts = pad_sequences(train_texts, maxlen=MAX_LENGTH)\n",
        "val_texts = pad_sequences(val_texts, maxlen=MAX_LENGTH)\n",
        "test_texts = pad_sequences(test_texts, maxlen=MAX_LENGTH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccXFe8PXAbF7",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "## Training RNN Model\n",
        "\n",
        "Layers:\n",
        "\n",
        "\n",
        "1. 2 GRU Layers (128 units)\n",
        "2. Dense Hidden Layer (32 units, ReLU activation)\n",
        "3. Dense Hidden Layer (100 units, RuLU activation)\n",
        "4. Sigmoid Output Layer\n",
        "\n",
        "Dense Layer\n",
        "\n",
        "\n",
        "```\n",
        "output = activation(dot(input, kernel) + bias)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "015d7Gb2BGud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_rnn_model():\n",
        "    sequences = layers.Input(shape=(MAX_LENGTH,))\n",
        "    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)\n",
        "    x = layers.CuDNNGRU(128, return_sequences=True)(embedded)\n",
        "    x = layers.CuDNNGRU(128)(x)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    x = layers.Dense(100, activation='relu')(x)\n",
        "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
        "    model = models.Model(inputs=sequences, outputs=predictions)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['binary_accuracy']\n",
        "    )\n",
        "    return model\n",
        "    \n",
        "rnn_model = build_rnn_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQG70ox3BPmT",
        "colab_type": "text"
      },
      "source": [
        "### Fitting the Model\n",
        "\n",
        "I am going to use mini-batch (size 128) Adam optimization algorithm\n",
        "\n",
        "Noted that this process may takes up to 30 minutes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H5u6_ZlBOta",
        "colab_type": "code",
        "outputId": "5e10d4ed-8995-41a6-e15a-85b6933b1540",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "rnn_model.fit(\n",
        "    train_texts, \n",
        "    train_labels, \n",
        "    batch_size=128,\n",
        "    epochs=1,\n",
        "    validation_data=(val_texts, val_labels), )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26157/26157 [==============================] - 2754s 105ms/step - loss: 0.1541 - binary_accuracy: 0.9412 - val_loss: 0.1293 - val_binary_accuracy: 0.9521\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1b733375c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA6MD9sLWjyd",
        "colab_type": "text"
      },
      "source": [
        "### Testing the Model\n",
        "\n",
        "There are multiple evaluation metrics that can be use. I choose to use Accuracy, F1 and ROC score\n",
        "On training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw45H-r6XvtB",
        "colab_type": "code",
        "outputId": "d6ddad04-0090-4da9-a9ad-de575f3515d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "preds = rnn_model.predict(train_texts)\n",
        "print('Accuracy score: {:0.4}'.format(accuracy_score(train_labels, 1 * (preds > 0.5))))\n",
        "print('F1 score: {:0.4}'.format(f1_score(train_labels, 1 * (preds > 0.5))))\n",
        "print('ROC AUC score: {:0.4}'.format(roc_auc_score(train_labels, preds)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9563\n",
            "F1 score: 0.9563\n",
            "ROC AUC score: 0.9904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWLkMsTOXx1u",
        "colab_type": "text"
      },
      "source": [
        "On testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0BxJqeRWqC1",
        "colab_type": "code",
        "outputId": "08e23df3-4c49-4d02-97e2-8efd28a4983e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "preds = rnn_model.predict(test_texts)\n",
        "print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))\n",
        "print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))\n",
        "print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score: 0.9509\n",
            "F1 score: 0.951\n",
            "ROC AUC score: 0.9887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpuIXzTbXU37",
        "colab_type": "text"
      },
      "source": [
        "We acheive relatively close score on both training and testing data. To further improve the model, we can train the model for longer (more epoch), train a bigger network, choose a better neural network architecture, and tune the hyperparameters\n",
        "\n",
        "\n",
        "### Saving the model\n",
        "\n",
        "We have to save both the model and the weight matrix of each hidden layers, so we don't have to train it again.\n",
        "\n",
        "To save the model we can use\n",
        "```\n",
        "model.save('my_model.h5')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-1u2uCNXb2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model.save('NLP_6088118_model-v2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lkGYkAfjABG",
        "colab_type": "text"
      },
      "source": [
        "And to save the weight we can use\n",
        "\n",
        "\n",
        "```\n",
        "model.save_weights('my_model_weights.h5')\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayQH4XT8jSe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_model.save_weights('NLP_6088118_model_weight-v2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sMPOAZ1jg2I",
        "colab_type": "text"
      },
      "source": [
        "To use the model you can use these lines of code\n",
        "\n",
        "\n",
        "```\n",
        "from keras.models import load_model\n",
        "\n",
        "model = load_model('my_model.h5')\n",
        "model.load_weights('my_model_weights.h5')\n",
        "```\n",
        "\n",
        "Now you can use the model to predict your input data.\n"
      ]
    }
  ]
}